{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f0c8803",
   "metadata": {},
   "source": [
    "# RecSys Challenge - EDA and Data Cleaning\n",
    "\n",
    "*Alvin Karanja*\n",
    "\n",
    "In this notebook, I will perform an exploratory data analysis (EDA) and any relevant data cleaning on the provided dataset. The goal is to understand the data, identify any issues, and prepare it for further analysis and modeling.\n",
    "\n",
    "To reproduce the results in this notebook, please ensure the relevant assignment data `yoochoose-buys.dat` and `yoochoose-clicks.dat` are located within a folder named `data` in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52872877",
   "metadata": {},
   "source": [
    "## 1. Preprocessing the Data\n",
    "\n",
    "The raw data is saved as a generic `.dat` file which can be confusing to work with. We can observe that the data is actually comma delimited, so we can read the data as a CSV file in pandas, additionally we specify column names in accordance with the documentation, setting columns to appropriate data types.\n",
    "\n",
    "Note that in parsing the time column, we keep the data timezone aware by 'casting' the time to UTC. This is important for any time series analysis we may perform later.\n",
    "\n",
    "The results are saved as new `.csv` files, which will be used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe474c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Destination file names\n",
    "destination_buys = 'data/yoochoose-buys.csv'\n",
    "src_buys = 'data/yoochoose-buys.dat'\n",
    "\n",
    "destination_clicks = 'data/yoochoose-clicks.csv'\n",
    "src_clicks = 'data/yoochoose-clicks.dat'\n",
    "\n",
    "# Check if data already exists\n",
    "if not os.path.exists(destination_buys) or not os.path.exists(destination_clicks):\n",
    "\n",
    "    buys_data_raw = pd.read_csv(src_buys)\n",
    "    clicks_data_raw = pd.read_csv(src_clicks)\n",
    "\n",
    "    # Assigning column names\n",
    "    buys_data_raw.columns = ['session_id', 'time', 'item_id', 'price', 'quantity']\n",
    "    clicks_data_raw.columns = ['session_id', 'time', 'item_id', 'category']\n",
    "\n",
    "    # Setting appropriate data types\n",
    "    buys_data_raw = buys_data_raw.astype({\n",
    "        'session_id': 'str',\n",
    "        'time': 'datetime64[ns, UTC]',\n",
    "        'item_id': 'str',\n",
    "        'price': 'float64',\n",
    "        'quantity': 'int64'\n",
    "    })\n",
    "\n",
    "    clicks_data_raw = clicks_data_raw.astype({\n",
    "        'session_id': 'str',\n",
    "        'time': 'datetime64[ns, UTC]',\n",
    "        'item_id': 'str',\n",
    "        'category': 'str'\n",
    "    })\n",
    "\n",
    "    # Saving the processed data to CSV files\n",
    "    buys_data_raw.to_csv('data/yoochoose-buys.csv', index=False)\n",
    "    clicks_data_raw.to_csv('data/yoochoose-clicks.csv', index=False)\n",
    "\n",
    "    # Clear the raw data variables \n",
    "    # to free up memory\n",
    "    del buys_data_raw\n",
    "    del clicks_data_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f7888f",
   "metadata": {},
   "source": [
    "Next we proceed to check the data for missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df52805d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1150752, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ca41b20c-acd0-47fd-a89c-73f2b95a6099",
       "rows": [
        [
         "session_id",
         "0"
        ],
        [
         "time",
         "0"
        ],
        [
         "item_id",
         "0"
        ],
        [
         "price",
         "0"
        ],
        [
         "quantity",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/plain": [
       "session_id    0\n",
       "time          0\n",
       "item_id       0\n",
       "price         0\n",
       "quantity      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buys_df = pd.read_csv(\n",
    "    destination_buys,\n",
    "    dtype={\n",
    "        'session_id': 'str',\n",
    "        'item_id': 'str',\n",
    "        'price': 'float64',\n",
    "        'quantity': 'int64'\n",
    "    },\n",
    "    parse_dates=['time'],\n",
    ")\n",
    "print(buys_df.shape)\n",
    "buys_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69a23a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33003943, 4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "078d2ac9-e31d-403a-ac15-efbbfc8ba5c7",
       "rows": [
        [
         "session_id",
         "0"
        ],
        [
         "time",
         "0"
        ],
        [
         "item_id",
         "0"
        ],
        [
         "category",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 4
       }
      },
      "text/plain": [
       "session_id    0\n",
       "time          0\n",
       "item_id       0\n",
       "category      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks_df = pd.read_csv(\n",
    "    destination_clicks,\n",
    "    dtype={\n",
    "        'session_id': 'str',\n",
    "        'item_id': 'str',\n",
    "        'category': 'str'\n",
    "    },\n",
    "    parse_dates=['time'],\n",
    ")\n",
    "print(clicks_df.shape)\n",
    "clicks_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdba07f",
   "metadata": {},
   "source": [
    "We observe that there are no missing values in the dataset and all columns are populated as expected. \n",
    "\n",
    "Next we inspect the date ranges for the data to understand the time period covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e2c029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clicks data date range: ('2014-04-01 03:00:00.124000+00:00', '2014-09-30 02:59:59.430000+00:00')\n",
      "Buys data date range: ('2014-04-01 03:05:31.743000+00:00', '2014-09-30 02:35:12.859000+00:00')\n"
     ]
    }
   ],
   "source": [
    "# Buys data date range\n",
    "buys_date_range = buys_df['time'].min(), buys_df['time'].max()\n",
    "clicks_date_range = clicks_df['time'].min(), clicks_df['time'].max()\n",
    "\n",
    "print(f\"Clicks data date range: {clicks_date_range}\")\n",
    "print(f\"Buys data date range: {buys_date_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21012ac",
   "metadata": {},
   "source": [
    "We note that the timelines for the data are consistent across the two datasets, covering the period from 2014-04-01 to 2014-09-30."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e822ed0",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "The data appears to be clean, with no missing values or obvious outliers. This allows us to focus on the implementation of the prediction algorithm without having to worry about data quality issues.\n",
    "\n",
    "### 2.1 Duplicate Data Observation\n",
    "\n",
    "In the data, we observe that there are some instances where the user appears to purchase the same item multiple times within a very short time frame (less than 1 minute). This may be due to several reasons such as:\n",
    "\n",
    "1. The user may be purchasing the same item multiple times within a single checkout session.\n",
    "2. System level logging artifacts where the same item is logged multiple times due to system retries or errors.\n",
    "\n",
    "![Duplicate Entries](assets/duplicate_data.png)\n",
    "\n",
    "Research into the dataset suggested that the first case is more likely, and the purpose of the dataset is to focus on producing a suitable prediction algorithm as opposed to tasks such as data cleaning or anomaly detection. To that end, we will not remove these duplicate entries from the dataset, as they may be relevant for the prediction algorithm.\n",
    "\n",
    "We refer to section 2 from the publication of the RecSys Challenge 2015 for this decision (Ben-Shimon et al., 2015), as there is no specific mention of any data cleaning preprocess necessary for the dataset, and it appears to be implied that the dataset is ready for use in prediction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c82702",
   "metadata": {},
   "source": [
    "## *References*\n",
    "\n",
    "- Ben-Shimon, D., Tsikinovsky, A., Friedmann, M., Shapira, B., Lior Rokach and Hoerle, J. (2015). RecSys Challenge 2015 and the YOOCHOOSE Dataset. doi:https://doi.org/10.1145/2792838.2798723."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
